\documentclass[titlepage]{article}

\usepackage[letterpaper,margin=1in,footskip=0.25in]{geometry}
\usepackage{fancyhdr}
\usepackage{csquotes}
\usepackage{tikz}
\usepackage{mdframed}
\usepackage{amsmath,amssymb}
\usepackage[nottoc,notlof]{tocbibind}
\usepackage{scrextend}
\usepackage[bottom]{footmisc}
\usepackage{titlesec}
\usepackage{wasysym}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage[toc]{glossaries}

\MakeOuterQuote{"}

\newmdenv[
    backgroundcolor=red!10,
    linewidth=0,
    innertopmargin=\topskip,
    innerbottommargin=\topskip
]{probint}
\newmdenv[
    backgroundcolor=yellow!80!red!10,
    linewidth=0,
    innertopmargin=\topskip,
    innerbottommargin=\topskip
]{prob}

\deffootnotemark{\textsuperscript{[\thefootnotemark]}}
\deffootnote[2em]{0em}{1.6em}{\textsuperscript{\thefootnotemark}}

\titleformat*{\subparagraph}{\itshape}

\makeglossaries
\newglossaryentry{markovmatrix}{
    name=markov matrix,
    description={An $n$-square matrix $A$ such that every entry $a_{ij}\in[0,1]$ and the sum of all entries $a_{ij}$ in each column $j$ is 1. Symbolically, $\sum_{i=1}^n a_{ij} = 1$ for all $j=1,2,\dots,n$}
}
\newglossaryentry{markovchain}{
    name=markov chain,
    description={``A[n] [algebraic] system that experiences transitions from one state to another according to certain probabilistic rules" \cite{bib:markovchains}}
}
\newglossaryentry{diagonalize}{
    name=diagonalize,
    description={To factor an $n$-square matrix $A$ into the form $S\Lambda S^{-1}$, where $S$ is an $n$-square matrix containing the eigenvectors $x_1,x_2,\dots,x_n$ (from left to right) of $A$ and $\Lambda$ is an $n$-square diagonal matrix containing the corresponding eigenvalues $\lambda_1,\lambda_2,\dots,\lambda_n$ (from top-left to bottom-right) of $A$}
}
\newglossaryentry{singular}{
    name=singular,
    description={Not invertible. Other characteristics of singular matrices include not being full column rank and having a nontrivial null space}
}
\newglossaryentry{permutationmatrix}{
    name=permutation matrix,
    description={A matrix with exactly one 1 in every row and column and zeroes everywhere else}
}
\renewcommand{\glsnamefont}{\makefirstuc}

\renewcommand{\labelenumii}{\theenumii.}

\newcounter{lemma}
\newcounter{theorem}
\newcounter{corollary}

\newenvironment{lemma}[2]{
    \setenumerate{itemindent=15pt}
    \refstepcounter{lemma}\label{#1}
    \paragraph{Lemma \thelemma}\hangindent=15pt #2

    \setlength{\leftskip}{15pt}
    \subparagraph{\hspace{-15pt}Proof}
}{

    \setlength{\leftskip}{0pt}
    \qed
}
\newenvironment{theorem}[2]{
    \setenumerate{itemindent=15pt}
    \refstepcounter{theorem}\label{#1}
    \paragraph{Theorem \thetheorem}\hangindent=15pt #2

    \setlength{\leftskip}{15pt}
    \subparagraph{\hspace{-15pt}Proof}
}{

    \setlength{\leftskip}{0pt}
    \qed
}
\newenvironment{corollary}[2]{
    \setenumerate{itemindent=15pt}
    \refstepcounter{corollary}\label{#1}
    \paragraph{Corollary \thecorollary}\hangindent=15pt #2

    \setlength{\leftskip}{15pt}
    \subparagraph{\hspace{-15pt}Proof}
}{

    \setlength{\leftskip}{0pt}
    \qed
}

\newcommand{\qed}{
    \begin{flushright}
        $\blacksquare$
    \end{flushright}
}
\newcommand{\dd}[2][]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\spn}[1]{\text{span}\left( #1 \right)}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\title{On Infinite Markov Chains}
\author{Steven Labalme}
\date{\today}

\begin{document}




\pagenumbering{gobble}
\maketitle



\pagenumbering{roman}
\tableofcontents
\listoffigures
\newpage



\pagenumbering{arabic}
\pagestyle{fancy}
\fancyhf{}
\rfoot{Labalme \thepage}
\renewcommand{\headrulewidth}{0pt}
\begin{center}
    \setcounter{secnumdepth}{0}
    \section{Abstract}
    \setcounter{secnumdepth}{3}
\end{center}
This paper was inspired by Problem 9 of the Chapter 6 review problems, which is restated below for convenience.
\vspace{8pt}

\begin{probint}
    \begin{description}
        \item[Problem 9] \hfill \\ Let $
            A =
            \begin{bmatrix}
                0.9 & 0.4\\
                0.1 & 0.6\\
            \end{bmatrix}
        $. Find the limiting value of $
            A^k
            \begin{bmatrix}
                3\\
                2\\
            \end{bmatrix}
        $ as $k\to\infty$.
    \end{description}
\end{probint}

However, to clarify the findings of this paper\footnote{So that $c\neq 1$ in Section \ref{ss2:applying}. If $c$ were to equal 1, the derived-by-inspection null space vector could be confused with the solution vector.}, Problem 9 will hereafter be considered to be the following.

\begin{prob}
    \begin{description}
        \item[Problem 9] \hfill \\ Let $
            A =
            \begin{bmatrix}
                0.9 & 0.4\\
                0.1 & 0.6\\
            \end{bmatrix}
        $. Find the limiting value of $
            A^k
            \begin{bmatrix}
                6\\
                4\\
            \end{bmatrix}
        $ as $k\to\infty$.
    \end{description}
\end{prob}

While Problem 9 presents as a straightforward \Gls{markovchain} problem, and it certainly can be solved as such, this paper explores a quicker, far simpler method. To begin, Problem 9 will be solved by following the standard \Gls{markovchain} procedure. Following that, empirical evidence will be assembled to suggest the presence of a simpler method, and this evidence will be fleshed out. After the validity of this new method is shown to extend to Problem 9, the tenets of the method will be rigorously derived.\par
Having been presented, employed, and justified, the new method will be compared and contrasted with the \Gls{markovchain} method. Lastly, a number of new areas of research opened up by the derivation of the new method will be explored.\par
Note that the techniques used in Section \ref{sse:markovchain} are adapted from in-class teachings, summarized in the author's class notes for Chapter 6: \cite{bib:ch6notes}. Note also that this paper will entirely ignore complex arithmetic.
\newpage



\section{Solving Problem 9 With a Markov Chain}\label{sse:markovchain}
Restate Problem 9 for convenience:

\begin{prob}
    \begin{description}
        \item[Problem 9] \hfill \\ Let $
            A =
            \begin{bmatrix}
                0.9 & 0.4\\
                0.1 & 0.6\\
            \end{bmatrix}
        $. Find the limiting value of $
            A^k
            \begin{bmatrix}
                6\\
                4\\
            \end{bmatrix}
        $ as $k\to\infty$.
    \end{description}
\end{prob}

The presence of both a \textbf{\Gls{markovmatrix}} ($A$) and of $A$ being raised to a power before mapping an initial condition vector evokes a \textbf{\Gls{markovchain}}. As such, the tools of \Glspl{markovchain} can be employed to evaluate Equation \ref{eqn:problem9}, which symbolically represents Problem 9.

\begin{equation}\label{eqn:problem9}
    \lim_{k\to\infty}
    \begin{bmatrix}
        0.9 & 0.4\\
        0.1 & 0.6\\
    \end{bmatrix}^k
    \begin{bmatrix}
        6\\
        4\\
    \end{bmatrix}
\end{equation}

To efficiently raise $A$ to an arbitrarily high power, \textbf{\gls{diagonalize}} it. The reason for the diagonalization is that due to the nature of the $A=S\Lambda S^{-1}$ factorization, $A^k=S\Lambda^kS^{-1}$. Indeed, diagonalization transforms the complex process of raising $A$ to a power into the relatively simple task of raising a diagonal matrix to a power and then recompiling the factorization.\par
To \gls{diagonalize} $A$, first find the eigenvalues, as follows.
\begin{align*}
    0 &=
    \begin{vmatrix}
        0.9-\lambda & 0.4\\
        0.1 & 0.6-\lambda\\
    \end{vmatrix}\\
    &= (0.9-\lambda)(0.6-\lambda)-(0.4)(0.1)\\
    &= 0.54-1.5\lambda+\lambda^2-0.04\\
    &= \lambda^2-1.5\lambda+0.5\\
    &= 2\lambda^2-3\lambda+1\\
    &= (2\lambda-1)(\lambda-1)
\end{align*}
\begin{align*}
    \lambda_1 &= 0.5&
        \lambda_2 &= 1
\end{align*}
Use the eigenvalues to find the eigenvectors, as follows\footnote{\label{fnt:inspection}Note that the sudden appearance of the eigenvectors in the following algebra is because inspection (basically guess and check) was used to find them. Inspection is a suitable method for simple instances, such as this 2-square example.}.
\begin{align*}
    0 &= (A-\lambda_1I)x_1&
        0 &= (A-\lambda_2I)x_2\\
    \begin{bmatrix}
        0\\
        0\\
    \end{bmatrix}    
    &= \left( 
        \begin{bmatrix}
            0.9 & 0.4\\
            0.1 & 0.6\\
        \end{bmatrix}
        -0.5
        \begin{bmatrix}
            1 & 0\\
            0 & 1\\
        \end{bmatrix}
    \right)
    \begin{bmatrix}
        x_{1_1}\\
        x_{1_2}\\
    \end{bmatrix}&
        \begin{bmatrix}
            0\\
            0\\
        \end{bmatrix}
        &= \left( 
            \begin{bmatrix}
                0.9 & 0.4\\
                0.1 & 0.6\\
            \end{bmatrix}
            -1
            \begin{bmatrix}
                1 & 0\\
                0 & 1\\
            \end{bmatrix}
        \right)
        \begin{bmatrix}
            x_{1_1}\\
            x_{1_2}\\
        \end{bmatrix}\\
    &=
    \begin{bmatrix}
        0.4 & 0.4\\
        0.1 & 0.1\\
    \end{bmatrix}
    \begin{bmatrix}
        x_{1_1}\\
        x_{1_2}\\
    \end{bmatrix}&
        &=
        \begin{bmatrix}
            -0.1 & 0.4\\
            0.1 & -0.4\\
        \end{bmatrix}
        \begin{bmatrix}
            x_{1_1}\\
            x_{1_2}\\
        \end{bmatrix}\\
    &=
    \begin{bmatrix}
        0.4 & 0.4\\
        0.1 & 0.1\\
    \end{bmatrix}
    \begin{bmatrix}
        -1\\
        1\\
    \end{bmatrix}&
        &=
        \begin{bmatrix}
            -0.1 & 0.4\\
            0.1 & -0.4\\
        \end{bmatrix}
        \begin{bmatrix}
            4\\
            1\\
        \end{bmatrix}
\end{align*}
\begin{align*}
    x_1 &=
    \begin{bmatrix}
        -1\\
        1\\
    \end{bmatrix}&
        x_2 &=
        \begin{bmatrix}
            4\\
            1\\
        \end{bmatrix}
\end{align*}
From the above information ($\lambda_1$, $\lambda_2$, $x_1$, and $x_2$), $A$ can be diagonalized as follows\footnote{Note that the $-\frac{1}{5}$ pertains to every entry of $S^{-1}$. It is factored out so that every entry of $S^{-1}$ is not a decimal.}.

\begin{equation}\label{eqn:diagonalization}
    A = -\frac{1}{5}
    \begin{bmatrix}
        -1 & 4\\
        1 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        0.5 & 0\\
        0 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        1 & -4\\
        -1 & -1\\
    \end{bmatrix}
\end{equation}

At this point, it is possible to return to the original task of evaluating Equation \ref{eqn:problem9}. Begin by substituting Equation \ref{eqn:diagonalization} into Equation \ref{eqn:problem9}, as follows.
\begin{equation*}
    \lim_{k\to\infty}
    \left( 
        -\frac{1}{5}
        \begin{bmatrix}
            -1 & 4\\
            1 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            0.5 & 0\\
            0 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            1 & -4\\
            -1 & -1\\
        \end{bmatrix}
    \right)^k
    \begin{bmatrix}
        6\\
        4\\
    \end{bmatrix}
\end{equation*}
As was discussed earlier, move the exponent into the factorization so that it only raises $\Lambda$ to the power $k$.
\begin{equation*}
    \lim_{k\to\infty} -\frac{1}{5}
    \begin{bmatrix}
        -1 & 4\\
        1 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        0.5 & 0\\
        0 & 1\\
    \end{bmatrix}^k
    \begin{bmatrix}
        1 & -4\\
        -1 & -1\\
    \end{bmatrix}
    \begin{bmatrix}
        6\\
        4\\
    \end{bmatrix}
\end{equation*}
Apply the fact that raising a diagonal matrix to a power is the same as raising each of its entries to that power.
\begin{equation*}
    \lim_{k\to\infty} -\frac{1}{5}
    \begin{bmatrix}
        -1 & 4\\
        1 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        0.5^k & 0\\
        0 & 1^k\\
    \end{bmatrix}
    \begin{bmatrix}
        1 & -4\\
        -1 & -1\\
    \end{bmatrix}
    \begin{bmatrix}
        6\\
        4\\
    \end{bmatrix}
\end{equation*}
It is now possible to evaluate the limit. While it may not be obvious what limit the entirety of the above expression approaches, all that is necessary is to evaluate the limits of $0.5^k$ and $1^k$ as $k\to\infty$, respectively, and then recompile the factorization. This can be done as follows.
\begin{align*}
    \lim_{k\to\infty} -\frac{1}{5}
    \begin{bmatrix}
        -1 & 4\\
        1 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        0.5^k & 0\\
        0 & 1^k\\
    \end{bmatrix}
    \begin{bmatrix}
        1 & -4\\
        -1 & -1\\
    \end{bmatrix}
    \begin{bmatrix}
        6\\
        4\\
    \end{bmatrix}
    &= -\frac{1}{5}
    \begin{bmatrix}
        -1 & 4\\
        1 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        0 & 0\\
        0 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        1 & -4\\
        -1 & -1\\
    \end{bmatrix}
    \begin{bmatrix}
        6\\
        4\\
    \end{bmatrix}\\
    &= -\frac{1}{5}
    \begin{bmatrix}
        -1 & 4\\
        1 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        0 & 0\\
        0 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        -10\\
        -10\\
    \end{bmatrix}\\
    &= -\frac{1}{5}
    \begin{bmatrix}
        -1 & 4\\
        1 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        0\\
        -10\\
    \end{bmatrix}\\
    &= -\frac{1}{5}
    \begin{bmatrix}
        -40\\
        -10\\
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        8\\
        2\\
    \end{bmatrix}
\end{align*}
The vector $
    \begin{bmatrix}
        8\\
        2\\
    \end{bmatrix}
$ is the final simplification of Equation \ref{eqn:problem9} and, therefore, the answer to Problem 9.
\newpage



\section{Solving Problem 9 More Concisely}\label{sse:concise}
\subsection{A New Method}
\subsubsection{Assembling the New Method}\label{ss2:assemble}
The limit given by Equation \ref{eqn:problem9} converges on an element of $\R^2$ as $k\to\infty$. But there is another, simpler way to think about this process. Because the limit converges, as $k$ increases, the change that each repeated application of $A$ causes to the product diminishes, i.e., the difference vector $Ax_0-x_0$ is greater in magnitude than the difference vector $A^2x_0-Ax_0$, and so on and so forth.\par
The consistently diminishing nature of the difference vector implies that at infinity, this difference vector is 0: symbolically, $AA^\infty x_0-A^\infty x_0=0$. If $x=A^\infty x_0$ is the solution vector (the evaluation of Equation \ref{eqn:problem9}), then substitution gives $Ax-x=0$, or
\begin{equation}\label{eqn:Axx}
    Ax = x
\end{equation}
This result actually makes quite a bit of sense --- if $x$ truly is the limit of repeated multiplications by $A$, it stands to reason that $A$ could no longer do anything to change it. Indeed, $A$ would be an identity map on $x$, and the above equation reflects that fact, as it can be restated $Ax=Ix$.\par
Equation \ref{eqn:Axx} provides a computationally far simpler method than Markov chains to find $x$. However, the solution to $Ax=x$ is the one-dimensional subspace $\spn{x}$, only one vector of which will be the solution to Problem 9. Thus, to pin down a single vector, it will be necessary to find a second equation describing this situation.\par
A good first place to look is at the initial conditions $x_0$, which have not yet been used. Since the solution most certainly depends on them, perhaps it is possible to use them to find the desired vector of $\spn{x}$, killing two birds with one stone. Indeed, finding a single solution depends on the initial conditions, but in a manner best visualized by applying the \Gls{markovchain} to a more tangible model, as follows.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        every path/.style={thick,->}
    ]
        \node [circle,draw,thin,fill=green!10] (PGH) {PGH}
            edge [out=100,in=130,loop] node[above]{0.9} ()
        ;
        \node [circle,draw,thin,fill=blue!10] (CLE) at (5,0) {CLE}
            edge [out=80,in=50,loop] node[above]{0.6} ()
        ;
        
        \draw (PGH) to[bend left=30] node[above]{0.1} (CLE);
        \draw (CLE) to[bend left=30] node[below]{0.4} (PGH);
    \end{tikzpicture}
    \caption{Modeling population changes with a Markov chain.}
    \label{fig:population}
\end{figure}

Consider matrix $A$ from Problem 9, but with the initial conditions adjusted to be 600,000 and 400,000 respectively. Figure \ref{fig:population} visualizes Problem 9 in the following ways.
\begin{enumerate}[itemsep=0pt]
    \item Imagine that 600,000 is the population of Pittsburgh (PGH) in year 0.
    \item Imagine that 400,000 is the population of Cleveland (CLE) in year 0.
    \begin{enumerate}
        \item Together, the populations are represented by the vector $
            x_0 =
            \begin{bmatrix}
                \text{PGH}\\
                \text{CLE}\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                600,000\\
                400,000\\
            \end{bmatrix}
        $.
    \end{enumerate}
    \item Every year, 90\% (0.9) of Pittsburgh's population remains in Pittsburgh, and 10\% (0.1) moves to Cleveland.
    \item Every year, 60\% (0.6) of Cleveland's population remains in Cleveland, and 40\% (0.4) make the right choice (\smiley{}).
    \begin{enumerate}
        \item Together, the movements are represented by the linear map $
            A =
            \begin{bmatrix}
                0.9 & 0.4\\
                0.1 & 0.6\\
            \end{bmatrix}
        $.
    \end{enumerate}
    \item In year $k$, the populations of each city are given by $A^kx_0$.
    \begin{enumerate}
        \item For example, in year 1, the populations are $
            \begin{bmatrix}
                \text{PGH}\\
                \text{CLE}\\
            \end{bmatrix}
            = A^1x_0 =
            \begin{bmatrix}
                0.9 & 0.4\\
                0.1 & 0.6\\
            \end{bmatrix}^1
            \begin{bmatrix}
                600,000\\
                400,000\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                700,000\\
                300,000\\
            \end{bmatrix}
        $.
    \end{enumerate}
    \item In the long run, the populations of each city are given by the limit of $A^kx_0$ as $k\to\infty$.
    \begin{enumerate}
        \item Eventually, the populations converge on $
            \begin{bmatrix}
                \text{PGH}\\
                \text{CLE}\\
            \end{bmatrix}
            = A^\infty x_0 =
            \begin{bmatrix}
                0.9 & 0.4\\
                0.1 & 0.6\\
            \end{bmatrix}^\infty
            \begin{bmatrix}
                600,000\\
                400,000\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                800,000\\
                200,000\\
            \end{bmatrix}
        $\footnote{Note that the final vector can be found by scaling the solution vector from Section \ref{sse:markovchain} by the same factor (100,000) that scaled $x_0$. This is further evidence that the solution depends on the initial conditions.}.
    \end{enumerate}
\end{enumerate}
In this model, it should be clear that no person is ever introduced or removed from the system --- they only move from one city to the other, or stay in place. This is reflected by the fact that the net number of people in every "year" given above is always one million --- $600,000+400,000=1,000,000$, $700,000+300,000=1,000,000$, and $800,000+200,000=1,000,000$.\par
Herein lies the key: Multiplying the initial conditions by a \Gls{markovmatrix} $A$ appears to preserve the sum of the values in $x_0$. Therefore, it is possible to formalize this second rule as follows, where each sum iterates through the $n$ entries in its respective vector.
\begin{equation}\label{eqn:sum}
    \sum_{i=1}^n x_{0_i} = \sum_{i=1}^n x_i
\end{equation}
With the results of Equations \ref{eqn:Axx} and \ref{eqn:sum}, solving Problem 9 is a trivial matter.

\subsubsection{Applying the New Method}\label{ss2:applying}
Restate Problem 9 for convenience:

\begin{prob}
    \begin{description}
        \item[Problem 9] \hfill \\ Let $
            A =
            \begin{bmatrix}
                0.9 & 0.4\\
                0.1 & 0.6\\
            \end{bmatrix}
        $. Find the limiting value of $
            A^k
            \begin{bmatrix}
                6\\
                4\\
            \end{bmatrix}
        $ as $k\to\infty$.
    \end{description}
\end{prob}

In parallel with Section \ref{ss2:assemble}, $A$ is a 2-square \Gls{markovmatrix} and $x_0$ will be the variable name for the initial conditions of the \Gls{markovchain} in Problem 9. Also, once again let $x$ be the sought-after solution vector.\par
Use Equation \ref{eqn:Axx} to solve for the subspace in which $x$ lies (see Footnote \ref{fnt:inspection}).
\begin{align*}
    x &= Ax\\
    0 &= Ax-x\\
    &= Ax-Ix\\
    &= (A-I)x\\
    \begin{bmatrix}
        0\\
        0\\
    \end{bmatrix}
    &= \left( 
        \begin{bmatrix}
            0.9 & 0.4\\
            0.1 & 0.6\\
        \end{bmatrix}
        -
        \begin{bmatrix}
            1 & 0\\
            0 & 1\\
        \end{bmatrix}
    \right)
    \begin{bmatrix}
        x_1\\
        x_2\\
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        -0.1 & 0.4\\
        0.1 & -0.4\\
    \end{bmatrix}
    \begin{bmatrix}
        x_1\\
        x_2\\
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        -0.1 & 0.4\\
        0.1 & -0.4\\
    \end{bmatrix}
    \begin{bmatrix}
        4c\\
        1c\\
    \end{bmatrix}
    ,\qquad c\in\R
\end{align*}

\begin{equation*}
    x = \spn{
        \begin{bmatrix}
            4\\
            1\\
        \end{bmatrix}
    }
\end{equation*}
Use Equation \ref{eqn:sum} to solve for a single $x$.
\begin{align*}
    \sum_{i=1}^n x_{0_i} &= \sum_{i=1}^n x_i\\
    6+4 &= 4c+1c\\
    c &= 2
\end{align*}
\begin{align*}
    x &=
    \begin{bmatrix}
        4(2)\\
        1(2)\\
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        8\\
        2\\
    \end{bmatrix}
\end{align*}
The vector $
    \begin{bmatrix}
        8\\
        2\\
    \end{bmatrix}
$ is the specific element of $
    \spn{
        \begin{bmatrix}
            4\\
            1\\
        \end{bmatrix}
    }
$ that is the solution to Problem 9.


\subsection{Rigorously Deriving the New Method}\label{sss:rigorous}
Let $A$ be an $n$-square \Gls{markovmatrix}, where $n\geq 2$ and $n\in\N$, and let $x_0\in\R^n$ be a vector of initial conditions for the \Gls{markovchain}.

\begin{lemma}{lma:A-I}{
    $A-I$ is a \textbf{\gls{singular}} matrix.
}
    By the definition of a \gls{markovmatrix}, the entries in each column of $A$ sum to 1. However, when the difference $A-I$ is taken, one entry of every column of $A$ decreases in value by 1. Thus, the entries in each column of $A-I$ sum to 0.\par
    If the sum of the entries in each column of $A-I$ is 0, every column $j$ (and every vector $v\in C(A-I)$) can be expressed in the following form.
    \begin{equation*}
        v =
        \begin{bmatrix}
            a_{1j}\\
            a_{2j}\\
            \vdots\\
            a_{(n-1)j}\\
            -a_{1j}-a_{2j}-\cdots-a_{(n-1)j}\\
        \end{bmatrix}
    \end{equation*}
    Notably, no vector \emph{not} in this form can be an element of $C(A-I)$, and, similarly, any vector not in this form would be an element of $N(A-I)$. Thus, proving that $A-I$ is \gls{singular} becomes a task of finding a vector $u\in\R^n$ that is not in the above form. The following is an example of such a vector.
    \begin{equation*}
        u =
        \begin{bmatrix}
            1\\
            1\\
            \vdots\\
            1\\
        \end{bmatrix}
    \end{equation*}
    It is obvious that the above vector is not in the form necessary for it to be an element of $C(A-I)$.\par
    As further proof that $u\notin C(A-I)$ and $u\in N(A-I)$, the fundamental theorem of linear algebra asserts that all elements of $C(A-I)$ are orthogonal to all elements of $N(A-I)$, or, alternatively, that the dot product of any element of the column space with any element of the null space is 0. Since $v$ encapsulates every possible element of $C(A-I)$, the following definitively proves that $u\notin C(A-I)$.
    \begin{align*}
        0 &= v\cdot u\\
        &=
        \begin{bmatrix}
            a_{1j}\\
            a_{2j}\\
            \vdots\\
            a_{(n-1)j}\\
            -a_{1j}-a_{2j}-\cdots-a_{(n-1)j}\\
        \end{bmatrix}
        \cdot
        \begin{bmatrix}
            1\\
            1\\
            \vdots\\
            1\\
        \end{bmatrix}\\
        &= (1)(a_{1j})+(1)(a_{2j})+\cdots+(1)(a_{(n-1)j})+(1)(-a_{1j}-a_{2j}-\cdots-a_{(n-1)j})\\
        &= 0
    \end{align*}
    Since there always exist elements of $\R^n$ (at least $\spn{u}$) in $N(A-I)$, $A-I$ is \gls{singular}.
\end{lemma}

\begin{lemma}{lma:Axx}{
    For all $A$, there exists a vector $x\in\R^n$ satisfying $Ax=x$.
}
    By Lemma \ref{lma:A-I}, the matrix $A-I$ has a null space. In other words, the following equation has at least one family of solutions.
    \begin{equation*}
        (A-I)x = 0
    \end{equation*}
    The above equation can be algebraically rearranged as follows, proving the lemma.
    \begin{align*}
        (A-I)x &= 0\\
        Ax-Ix &= 0\\
        Ax &= Ix\\
        Ax &= x
    \end{align*}
\end{lemma}

Lemmas \ref{lma:A-I} and \ref{lma:Axx} hold true for any \Gls{markovmatrix} $A$. However, before tackling convergence, it is necessary to point out that not all \Glspl{markovchain} converge. For example, if $A$ is the following \textbf{\gls{permutationmatrix}}, the \Gls{markovchain} diverges periodically for infinitely many (but not all) $x_0$.

\begin{equation*}
    \begin{bmatrix}
        0 & 1 & 0\\
        1 & 0 & 0\\
        0 & 0 & 1\\
    \end{bmatrix}
\end{equation*}

\begin{theorem}{trm:xislimit}{
    The solution to $\lim_{k\to\infty}A^kx_0$ lies in $\spn{x}$, where $x$ satisfies $Ax=x$, for any $x_0$ if and only if the limit converges.
}
    If $\lim_{k\to\infty}A^kx_0$ converges, the difference vector $A^{k+1}x_0-A^kx_0$ between an instance $A^kx_0$ and the next instance $A^{k+1}x_0$ converges to 0. The convergence of the distance vector implies the validity of the following algebra.
    \begin{align*}
        0 &= \lim_{k\to\infty} \left( A^{k+1}x_0-A^kx_0 \right)\\
        &= \lim_{k\to\infty} \left( AA^kx_0-IA^kx_0 \right)\\
        &= \lim_{k\to\infty} (A-I)A^kx_0\\
        &= (A-I)\lim_{k\to\infty} A^kx_0
    \end{align*}\par
    By the above algebra, $\lim_{k\to\infty} A^kx_0$ is an element of $N(A-I)$. If $\lim_{k\to\infty} A^kx_0\in N(A-I)$, Lemma \ref{lma:Axx} guarantees that it lies in $\spn{x}$, where $x$ satisfies $Ax=x$. 
\end{theorem}

Theorem \ref{trm:xislimit} concludes the proof of Equation \ref{eqn:Axx}. All that is left at this point is to prove Equation \ref{eqn:sum}.

\begin{lemma}{lma:sum}{
    The sum of the entries in $x_0$ equals the sum of the entries in $Ax_0$. Symbolically, the following equation is true for any $A$ and $x_0$: $\sum_{i=1}^nx_{0_i}=\sum_{i=1}^n\left( Ax_0 \right)_i$.
}
    Let $A$ and $x_0$ be visualized as follows.
    \begin{align*}
        A &=
        \begin{bmatrix}
            a_{11} & a_{12} & \cdots & a_{1n}\\
            a_{21} & a_{22} & \cdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \cdots & a_{nn}\\
        \end{bmatrix}&
            x_0 &=
            \begin{bmatrix}
                x_{0_1}\\
                x_{0_2}\\
                \vdots\\
                x_{0_n}
            \end{bmatrix}
    \end{align*}
    Then $Ax_0$ is given by the following.
    \begin{align*}
        Ax_0 &=
        \begin{bmatrix}
            a_{11} & a_{12} & \cdots & a_{1n}\\
            a_{21} & a_{22} & \cdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \cdots & a_{nn}\\
        \end{bmatrix}
        \begin{bmatrix}
            x_{0_1}\\
            x_{0_2}\\
            \vdots\\
            x_{0_n}\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            a_{11}x_{0_1}+a_{12}x_{0_2}+\cdots+a_{1n}x_{0_n}\\
            a_{21}x_{0_1}+a_{22}x_{0_2}+\cdots+a_{2n}x_{0_n}\\
            \vdots\\
            a_{n1}x_{0_1}+a_{n2}x_{0_2}+\cdots+a_{nn}x_{0_n}\\
        \end{bmatrix}
    \end{align*}
    With both $x_0$ and $Ax_0$ defined, algebraically transform one of the expressions in the hypothesis into the other, as follows\footnote{Note that the transition from the second to the third line of the following derivation is legal by the definition of a \Gls{markovmatrix} --- every sum $a_{1j}+\cdots+a_{nj}$ is the sum of the values in a column $j$ of $A$, which is, by definition, equal to 1.}.
    \begin{align*}
        \sum_{i=1}^n\left( Ax_0 \right)_i &= (a_{11}x_{0_1}+\cdots+a_{1n}x_{0_n})+(a_{21}x_{0_1}+\cdots+a_{2n}x_{0_n})+\cdots+(a_{n1}x_{0_1}+\cdots+a_{nn}x_{0_n})\\
        &= (a_{11}+\cdots+a_{n1})x_{0_1}+(a_{12}+\cdots+a_{n2})x_{0_2}+\cdots+(a_{1b}+\cdots+a_{nb})x_{0_n}\\
        &= 1x_{0_1}+1x_{0_2}+\cdots+1a_{0_n}\\
        &= \sum_{i=1}^nx_{0_i}
    \end{align*}
\end{lemma}

\begin{theorem}{trm:suminduction}{
    The sum of the entries in $x_0$ equals the sum of the entries in $A^kx_0$ for any $k\in\N$.
}
    The following will be an induction proof.\\
    Basis step: $\sum_{i=1}^n (Ax_0)_i \stackrel{\checkmark}{=} \sum_{i=1}^n x_{0_i}$ (guaranteed by Lemma \ref{lma:sum}).\\
    Induction hypothesis: $\sum_{i=1}^n (A^kx_0)_i = \sum_{i=1}^n x_{0_i}$.\\
    Induction step:
    \begin{align*}
        \sum_{i=1}^n (A^{k+1}x_0)_i &\stackrel{?}{=} \sum_{i=1}^n (Ax_0)_i\\
        \sum_{i=1}^n (A^{k+1}x_0)_i &\stackrel{\checkmark}{=} \sum_{i=1}^n x_{0_i}
    \end{align*}
\end{theorem}

Although Theorem \ref{trm:suminduction} does not directly treat the infinite case, stating that the sum of the entries never changes under repeated multiplications by $A$ implies that the sum of the entries never changes as $k\to\infty$, proving Equation \ref{eqn:sum}.\par
Together, Theorems \ref{trm:xislimit} and \ref{trm:suminduction} guarantee that the method of solving Problem 9 used in Section \ref{ss2:applying} stands on solid logical footing.
\newpage



\section{Comparing and Contrasting the Two Methods}
\Glspl{markovchain} are useful not only for their prediction of long-run behavior, but for their prediction of short-term behavior, too. For example, in the population model discussed in association with Figure \ref{fig:population}, the model's 1, 2, or 5-year predictions are likely far more useful than its long-run behavior; after all, those rates of population movement are unlikely to remain static for infinitely many years.\par
However, when determining the long-run behavior of a convergent \Gls{markovchain}, the new method is more than useful. It eliminates the need for the time-consuming process of diagonalization, removes the need for limits entirely, and avoids the lengthy process of recompiling the factorization.\par
Most importantly, the new method is a gateway to discovering and proving fascinating results on the nature of infinite \Glspl{markovchain}, some of which are discussed in the next section.
\newpage



\section{Diving Deeper}
The method of evaluating a convergent, infinite \Gls{markovchain} discussed in Section \ref{sse:concise} opens up several new, fascinating ways to learn about the properties of Markov matrices. Several of these will be explored below as corollaries to the lemmas and theorems of Section \ref{sss:rigorous}.\par
Let $A$ be an $n$-square \Gls{markovmatrix}, where $n\geq 2$ and $n\in\N$. Let $x\in\R^n$ satisfy $Ax=x$. Let $x_0\in\R^n$ be a vector of initial conditions for the \Gls{markovchain} such that $\lim_{k\to\infty}A^kx_0$ converges.

\begin{corollary}{cly:1iseigenvalue}{
    For all $A$, there exists an eigenvalue $\lambda_i$, where $\lambda_i=1$.
}
    By Lemma \ref{lma:Axx}, there always exists a vector $x$. By definition, an eigenvalue (of an arbitrary $m$-square matrix $B$) is a scalar $\lambda\in\R$ satisfying $By=\lambda y$, where $y\in\R^m$.\par
    The parallel between $Ax=x$, or $Ax=1x$, and $By=\lambda y$ is clear. Since $Ax=1x$ is always solvable, $\lambda_i=1$ must be an eigenvalue of $A$.
\end{corollary}

It is worth noting that since 1 is an eigenvalue of $A$, $x$ is an eigenvector. In other words, $\lim_{k\to\infty}A^kx_0$ is an eigenvector of $A$. Eigenvectors are also known as "characteristic vectors," and the appearance of one as the solution to $\lim_{k\to\infty}A^kx_0$ is yet another surprising expression of their characterization of $A$.\par
For the next corollary, let $P_A(\lambda)$ denote the characteristic polynomial of $A$.

\begin{corollary}{cly:dividingPA}{
    $P_A(\lambda)$ is evenly divisible by $(\lambda-1)$.
}
    By definition, the roots of $P_A(\lambda)$ are equal to the eigenvalues of $A$. By Corollary \ref{cly:1iseigenvalue}, 1 is an eigenvalue of any $A$ and, therefore, is a root of $P_A(\lambda)$. For any root $r$ of a polynomial $P(x)$, $(x-r)$ is a factor of $P(x)$. Since 1 is a root of $P_A(\lambda)$, $(\lambda-1)$ is a factor of $P_A(\lambda)$, which implies that $P_A(\lambda)$ is evenly divisible by $(\lambda-1)$.
\end{corollary}

Let $A_1$ and $A_2$ be $n$-square Markov matrices. For the purpose of the following proofs, it is necessary to define the following notation\footnote{I have not found a canonical expression of this idea, so I invented my own notation.}: If $u_1,u_2$ are vectors in the same dimension, let $u_1\cup u_2$ denote the matrix created by horizontally concatenating $u_1$ and $u_2$. For example,
\begin{equation*}
    u_1 =
    \begin{bmatrix}
        1\\
        2\\
        3\\
    \end{bmatrix},\
    u_2 =
    \begin{bmatrix}
        4\\
        5\\
        6\\
    \end{bmatrix}
    \Rightarrow u_1\cup u_2 =
    \begin{bmatrix}
        1 & 4\\
        2 & 5\\
        3 & 6\\
    \end{bmatrix}
\end{equation*}

\begin{corollary}{cly:product}{
    The product $A_1A_2$ is a \Gls{markovmatrix}.
}
    Consider two matrices $B$ and $C$ such that the product $BC$ exists. Due to the manner in which multiplying two matrices is defined, $B$ maps each column vector $c_i$ of $C$ to its corresponding column vector $bc_i$ of $BC$ in the same way that it would if $c_i$ were an independent vector. In other words, the fact that the column vectors of $C$ are all united in a single matrix does not change how $B$ acts on them.\par
    Let $a_{2_i}$ be a column vector of $A_2$. Because of the above ruminations on the mechanics of the matrix product, the following holds.
    \begin{equation*}
        A_1A_2 = A_1a_{2_1} \cup A_1a_{2_2} \cup\cdots\cup A_1a_{2_n}
    \end{equation*}
    By Lemma \ref{lma:sum}, the sum of the entries of every vector $A_1a_{2_i}$ is 1. Additionally, since every entry in $A_1$ and $A_2$ is positive, every entry in $A_1A_2$ is positive. Therefore, $A_1A_2$ is a \Gls{markovmatrix}.
\end{corollary}

\begin{corollary}{cly:noInfinite}{
    There exists no $A$ such that the entries of $A^k$ approach infinity as $k\to\infty$.
}
    Since the product of two Markov matrices is also a \Gls{markovmatrix} (as guaranteed by Corollary \ref{cly:product}), then $A^k$ is a \Gls{markovmatrix} for any $k\in\N$. Since the entries of any \Gls{markovmatrix} will always be finite values in the range $[0,1]$, the entries of $A^k$ can never be infinite, even as $k\to\infty$.
\end{corollary}

Let $A^\infty$ denote\footnote{For convenience only.} $\lim_{k\to\infty}A^k$ for all $A$ such that said limit exists.

\begin{corollary}{cly:sameNVectors}{
    Every column vector $v_1,v_2,\dots,v_n$ composing $A^\infty$ equals the element of $\spn{x}$ whose entries sum to 1.
}
    By Theorem \ref{trm:xislimit}, $\lim_{k\to\infty}A^kx_0$ is an element of $\spn{x}$. By Theorem \ref{trm:suminduction}, the sum of the entries in $x_0$ equals the sum of the entries in $\lim_{k\to\infty}A^kx_0$. Using an arbitrary matrix $B$ to map an arbitrary matrix $C$ (taking the product $BC$) has the same effect as using $B$ to map every column vector $c_i$ in $C$ individually and then concatenating the products $Bc_i$.\par
    From these three assertions, the following algebra can be constructed.
    \begin{align*}
        A^\infty &= \lim_{k\to\infty} A^k\\
        &= \lim_{k\to\infty} A^kA\\
        &= \left( \lim_{k\to\infty} A^k \right)\left( v_1 \cup v_2 \cup\cdots\cup v_n \right)\\
        &= \lim_{k\to\infty} A^kv_1 \cup \lim_{k\to\infty} A^kv_2 \cup\cdots\cup \lim_{k\to\infty} A^kv_n\\
        &= x \cup x \cup\cdots\cup x
    \end{align*}\par
    Although the symbol $x$ is used in the conclusion of the above algebra, note that it is not just any element of $\spn{x}$, but specifically the one whose entries sum to 1 that is represented. It is this element of $\spn{x}$ because the sum of the entries of $v_i$ is 1, implying by Theorem \ref{trm:suminduction} that the sum of the entries of $x=\lim_{k\to\infty}A^kv_i$ is 1.
\end{corollary}

\begin{corollary}{cly:AinftySingular}{
    $A^\infty$ is a \gls{singular} matrix.
}
    Corollary \ref{cly:sameNVectors} guarantees that every column vector of $A^\infty$ is identical. Therefore, $\dim C(A^\infty)=1$. Since $1<n$ by the definition of $A$, $A$ is \gls{singular}.
\end{corollary}

After Corollaries \ref{cly:sameNVectors} and \ref{cly:AinftySingular}, it is clear that $A^\infty$ has some interesting properties. To better visualize the properties of $A^\infty$ consider the case where $A$ is the matrix defined by Problem 9. In this case, by Corollary \ref{cly:sameNVectors}, $A^\infty$ is given by the following.
\begin{equation*}
    A^\infty =
    \begin{bmatrix}
        0.8 & 0.8\\
        0.2 & 0.2\\
    \end{bmatrix}
\end{equation*}
The above example actually makes quite a bit of sense. By Theorem \ref{trm:xislimit}, $A^\infty$ must be linear map from every $x_0$ to an element of $
    \spn{
        \begin{bmatrix}
            4\\
            1\\
        \end{bmatrix}
    }
$. Since matrices map vectors onto their own column space, $C(A)$ should equal $
\spn{
    \begin{bmatrix}
        4\\
        1\\
    \end{bmatrix}
}
$, as it does. Additionally, by Theorem \ref{trm:suminduction}, $A^\infty$ must preserve the sum of the entries in any vector it maps. By Lemma \ref{lma:sum}, it will. Building off of Theorem \ref{trm:suminduction} and Lemma \ref{lma:sum}, Corollary \ref{cly:product} implies that $A^\infty$ will be a \Gls{markovmatrix}, and indeed it is. Furthermore, expanding upon the earlier observations that $\lim_{k\to\infty}A^kx_0$ is an eigenvector of $A$, it is worth noting that $A^\infty$ is entirely composed of repeats of one of its eigenvectors. Lastly, as was referenced in Section \ref{ss2:assemble}, $AA^\infty$ should equal $A^\infty$, the rational being that one more application of $A$ should do nothing to change the infinitely many already performed. Indeed, because $A^\infty$ is composed entirely of vectors that $A$ scales by 1 (does not change), $AA^\infty$ will equal $A^\infty$.\par
Among all of the observations of the nature of $A^\infty$, one stands out as the most important: Despite the fact that $A$ is raised to the infinite power to generate $A^\infty$, every entry of $A^\infty$ is finite (because every entry of $x$ is finite). This finding will be instrumental in proving the next corollary.\par

\begin{corollary}{cly:eigenvalueRange}{
    All eigenvalues of $A$ are elements of $[-1,1]$.
}
    Suppose there exists an eigenvalue $\lambda$ of $A$ that is not an element of $[-1,1]$. $\lambda$ has corresponding eigenvector $y$. By the definition of eigenvalues and eigenvectors, the following equation holds.
    \begin{equation*}
        Ay = \lambda y
    \end{equation*}\par
    Consider what happens to both sides of the above equation when $A$ is raised to a power. The following algebra suggests an answer.
    \begin{align*}
        Ay &= \lambda y\\
        A^2y &= \lambda(Ay)\\
        A^2y &= \lambda(\lambda y)\\
        A^2y &= \lambda^2y
    \end{align*}
    Based on the above result, an induction proof can be constructed to demonstrate the following\footnote{Due to the simplicity of the finding and the existence of a induction proof in this paper already (Theorem \ref{trm:suminduction}), such a proof will be omitted.}.
    \begin{equation*}
        A^ky = \lambda^ky
    \end{equation*}
    At this point, it is necessary to consider the case where $\lim_{k\to\infty}A^ky$ exists and where $\lim_{k\to\infty}A^ky$ does not exist separately.\par
    \textsc{Case 1: $A^\infty$ exists}: Corollary \ref{cly:sameNVectors} implies, as was noted directly before this proof began, that should $A^\infty$ exist, all of its entries will be finite. Because $A^\infty$ is a finite matrix, $\lim_{k\to\infty}A^ky=A^\infty y$ will also be finite. On the other hand, $\lambda>1$ or $\lambda<-1$ implies that $\lambda^k$ will either grow exponentially or grow exponentially while oscillating between positive and negative values. Either way, $\lim_{k\to\infty}\lambda^ky$ must be infinite in nature. However, since the finite $\lim_{k\to\infty}A^ky$ must equal the infinite $\lim_{k\to\infty}\lambda^ky$, a contradiction has been reached.\par
    \textsc{Case 2: $A^\infty$ does not exist}: If $\lim_{k\to\infty}A^ky$ does not converge, then $A^k$ must diverge by oscillation because Corollary \ref{cly:noInfinite} guarantees that it cannot diverge to infinity. Additionally, if $A^k$ oscillates as $k$ increases, Corollary \ref{cly:product} guarantees that it oscillates between finite matrices. By the same argument used in \textsc{Case 1}, since $A^ky$ is necessarily finite for any $k$ but $\lambda^ky$ approaches infinity as $k\to\infty$, a contradiction has been reached.
\end{corollary}

Corollary \ref{cly:eigenvalueRange} implies something interesting regarding the determinant of $A$.

\begin{corollary}{cly:determinantRange}{
    For all $A$, $|A|\in[-1,1]$.
}
    One definition of the determinant is the product of the eigenvectors. Corollary \ref{cly:eigenvalueRange} guarantees that every eigenvector $\lambda$ is an element of $[-1,1]$. Since a product consisting only of elements in the range $[-1,1]$ will be in the range $[-1,1]$, $|A|\in[-1,1]$.
\end{corollary}

\begin{corollary}{cly:determinantToConvergence}{
    If $|A|=1$ or $|A|=-1$, then $A^\infty$ does not exist.
}
    In addition to some previous results, two properties of determinants will be required. Consider two square matrices $B$ and $C$ of identical dimensions.
    \begin{enumerate}
        \item $|B|=0$ if and only if $B$ is \gls{singular}.
        \item If $B\cdot C=BC$, then $|B|\cdot|C|=|BC|$.
    \end{enumerate}
    Importantly, property 2 implies that $|B^2|=|B|^2$ and, by induction, that $|B^k|=|B|^k$.\par
    For the body of the proof, consider the two cases separately.\par
    \textsc{Case 1: $|A|=1$}: Suppose $A^\infty$ exists. As $k\to\infty$ for $A^k$, $|A^k|$ remains 1 by the following algebra.
    \begin{align*}
        |A^k| &= |A|^k\\
        &= 1^k\\
        &= 1
    \end{align*}
    Thus, $|A^\infty|$ is 1. Since $|A^\infty|\neq 0$, property 1 asserts that $A^\infty$ is not \gls{singular}. However, Corollary \ref{cly:AinftySingular} guarantees that $A^\infty$ is singular, and a contradiction has been reached.\par
    \textsc{Case 2: $|A|=-1$}: Suppose $A^\infty$ exists. As $k\to\infty$ for $A^k$, $|A^k|$ is either 1 or $-1$. Thus, $|A^\infty|$ is either 1 or $-1$. Since a single matrix can only have a single determinant, a contradiction has been reached.
\end{corollary}

\begin{corollary}{cly:determinantExclusive9}{
    If $|A|\in(-1,0)\cup(0,1)$, then $A^\infty$ exists.
}
    If the determinant is in this range, then raising $A$ to a power causes the determinant to decrease exponentially. If the determinant is decreasing exponentially, the matrix is not oscillating. Since $A^k$ is not oscillating and cannot be growing exponentially (Corollary \ref{cly:noInfinite}) as $k\to\infty$, it must be converging.
\end{corollary}

The importance of Corollaries \ref{cly:determinantToConvergence} and \ref{cly:determinantExclusive9} is that they provide a simple way to predict whether or not a \Gls{markovchain} will converge on a steady state\footnote{Although I believe that Corollary \ref{cly:determinantExclusive9} can be extended for the case where $|A|=0$, I am unsure of how to prove it.}.

\begin{corollary}{cly:eigenvectorSum0}{
    The sum of the entries in every eigenvector of $A$ outside of $\spn{x}$ equals 0.
}
    Let $y$ be an eigenvector of $A$ outside of $\spn{x}$. By the definition of an eigenvector, $A$ can only map $y$ to another element of $\spn{y}$. Since $\spn{x}$ encapsulates all eigenvectors of $A$ with corresponding eigenvalue 1 (all solutions to $Ax=x$) and the eigenvalues of $A$ lie strictly in the range $[-1,1]$ (Corollary \ref{cly:eigenvalueRange}), the eigenvalue $\lambda$ corresponding to $y$ is an element of $[-1,1)$. At this point, it is necessary to begin casework.\par
    \textsc{Case 1: $\lambda\in(-1,1)$}: If $\lambda\in(-1,1)$, $A^ky=\lambda^ky=0$ as $k\to\infty$. Thus, at infinity, $y$ will have been scaled to 0, so the sum of its elements will be 0. However, Theorem \ref{trm:suminduction} guarantees that $A$ cannot change the sum of the entries in $y$, and since the scaling of $y$ to 0 was accomplished via repeatedly multiplying $y$ by $A$, only, the sum of the entries in $y$ is 0.\par
    \textsc{Case 2: $\lambda=-1$}: If $\lambda=-1$, then $Ay=-y$. Additionally, Theorem \ref{trm:suminduction} guarantees that $A$ does not change the sum of the entries in $y$. Thus, the following algebra holds.
    \begin{align*}
        \sum_{i=0}^n y_i &= \sum_{i=0}^n (Ay)_i\\
        \sum_{i=0}^n y_i &= \sum_{i=0}^n -y_i\\
        \sum_{i=0}^n y_i &= -\sum_{i=0}^n y_i\\
        2\sum_{i=0}^n y_i &= 0\\
        \sum_{i=0}^n y_i &= 0
    \end{align*}
\end{corollary}

Corollary \ref{cly:eigenvectorSum0} is rather difficult to visualize, so, as per usual, a relation to Problem 9 will be invoked. As was derived in Section \ref{sse:markovchain}, the two eigenvectors of $A$ (as defined by Problem 9) are as follows.
\begin{align*}
    x_1 &=
    \begin{bmatrix}
        -1\\
        1\\
    \end{bmatrix}&
        x_2 &=
        \begin{bmatrix}
            4\\
            1\\
        \end{bmatrix}
\end{align*}
$x_2$ is the parallel to $x$, as been explored ad nauseam, so it is outside the scope of Corollary \ref{cly:eigenvectorSum0}. However, the entries of $x_1$ ($-1$ and 1) do indeed sum to $-1+1=0$.\par
The above example pertains to \textsc{Case 1} in the proof of Corollary \ref{cly:eigenvectorSum0}. However, let's consider an example that pertains to \textsc{Case 2} as well. The following matrix was briefly referenced in Section \ref{sss:rigorous} as an example of a matrix that diverges periodically when raised to a power $k$ as $k\to\infty$.
\begin{equation*}
    \begin{bmatrix}
        0 & 1 & 0\\
        1 & 0 & 0\\
        0 & 0 & 1\\
    \end{bmatrix}
\end{equation*}
The eigenvalues and eigenvectors of this matrix are as follows.
\begin{align*}
    \lambda_1 &= 1&
        \lambda_2 &= 1&
            \lambda_3 &= -1\\
    x_1 &=
    \begin{bmatrix}
        0\\
        0\\
        1\\
    \end{bmatrix}&
        x_2 &=
        \begin{bmatrix}
            1\\
            1\\
            0\\
        \end{bmatrix}&
            x_3 &=
            \begin{bmatrix}
                -1\\
                1\\
                0\\
            \end{bmatrix}
\end{align*}
In this case, $\spn{x}$ is two dimensional, including $x_1$ and $x_2$. However, the entries of the one vector ($x_3$) which lies outside of $\spn{x}$ still sum to 0.
\newpage



\printglossaries
\newpage



\bibliography{MarkovMeditations}
\bibliographystyle{apalike}




\end{document}